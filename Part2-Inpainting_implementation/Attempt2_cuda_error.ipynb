{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/devdastl/EVA-8_Capstone_Assignment/blob/main/Part2-Inpainting_implementation/Attempt2_cuda_error.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install python dependecy required for the inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yblwvSMHb3N",
        "outputId": "0e391979-48c8-4355-ddd7-4e9bbcb07b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.14.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers\n",
        "!pip install transformers\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import python depedency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l2fc9zJpHaWv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from diffusers import AutoencoderKL, DDIMScheduler, DiffusionPipeline, PNDMScheduler, UNet2DConditionModel, DPMSolverMultistepScheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download VAE trained models from `runwayml/stable-diffusion-inpainting`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJwW6OnHaWx",
        "outputId": "ed52bdd4-cf7d-4fa5-da7e-500ced3a03bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FrozenDict([('in_channels', 3),\n",
              "            ('out_channels', 3),\n",
              "            ('down_block_types',\n",
              "             ['DownEncoderBlock2D',\n",
              "              'DownEncoderBlock2D',\n",
              "              'DownEncoderBlock2D',\n",
              "              'DownEncoderBlock2D']),\n",
              "            ('up_block_types',\n",
              "             ['UpDecoderBlock2D',\n",
              "              'UpDecoderBlock2D',\n",
              "              'UpDecoderBlock2D',\n",
              "              'UpDecoderBlock2D']),\n",
              "            ('block_out_channels', [128, 256, 512, 512]),\n",
              "            ('layers_per_block', 2),\n",
              "            ('act_fn', 'silu'),\n",
              "            ('latent_channels', 4),\n",
              "            ('norm_num_groups', 32),\n",
              "            ('sample_size', 256),\n",
              "            ('scaling_factor', 0.18215),\n",
              "            ('_class_name', 'AutoencoderKL'),\n",
              "            ('_diffusers_version', '0.6.0.dev0'),\n",
              "            ('_name_or_path', 'runwayml/stable-diffusion-inpainting')])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"vae\")\n",
        "vae.config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download and instantiate other trained models for:\n",
        "- stable duffusion tokenizer\n",
        "- CLIP text_encoder\n",
        "- Special UNET trained with 9 input channels and 4 output channel  \n",
        "- scheduler for adding noise\n",
        "- feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reMWe3GVHaWx",
        "outputId": "0cadcd60-7ede-4267-da77-9e9e38749d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"text_encoder\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"unet\")\n",
        "scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"scheduler\")\n",
        "#scheduler = PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"scheduler\")\n",
        "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"feature_extractor\")\n",
        "safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"safety_checker\")\n",
        "\n",
        "#scheduler = Union[scheduler1, scheduler2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now load the model in cuda device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT3qizNEHaWy",
        "outputId": "2bf4673b-3e33-4081-9617-2ba647c8c62b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StableDiffusionSafetyChecker(\n",
              "  (vision_model): CLIPVisionModel(\n",
              "    (vision_model): CLIPVisionTransformer(\n",
              "      (embeddings): CLIPVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "        (position_embedding): Embedding(257, 1024)\n",
              "      )\n",
              "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (encoder): CLIPEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x CLIPEncoderLayer(\n",
              "            (self_attn): CLIPAttention(\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): CLIPMLP(\n",
              "              (activation_fn): QuickGELUActivation()\n",
              "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "vae.to(device)\n",
        "text_encoder.to(device)\n",
        "unet.to(device)\n",
        "#feature_extractor.to(device)\n",
        "safety_checker.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "slyCm31HHaWy"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "code for preprocessing images and mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OdIma_ExHaWz"
      },
      "outputs": [],
      "source": [
        "def make_batch(image, mask, device):\n",
        "    image = np.array(Image.open(image).convert(\"RGB\"))\n",
        "    image = image.astype(np.float32)/255.0\n",
        "    image = image[None].transpose(0,3,1,2)\n",
        "    image = torch.from_numpy(image)\n",
        "\n",
        "    mask = np.array(Image.open(mask).convert(\"L\"))\n",
        "    mask = mask.astype(np.float32)/255.0\n",
        "    mask = mask[None,None]\n",
        "    mask[mask < 0.5] = 0\n",
        "    mask[mask >= 0.5] = 1\n",
        "    mask = torch.from_numpy(mask)\n",
        "\n",
        "    masked_image = (1-mask)*image\n",
        "\n",
        "    batch = {\"image\": image, \"mask\": mask, \"masked_image\": masked_image}\n",
        "    for k in batch:\n",
        "        batch[k] = batch[k].to(device=device)\n",
        "        batch[k] = batch[k]*2.0-1.0\n",
        "    return batch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBOdUilhHaW0"
      },
      "source": [
        "## Variables for inpainting "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "setup variables for inpainting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XfYoJKPDHaW0"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "num_inference_steps=50\n",
        "generator=None\n",
        "strength=0.8\n",
        "prompt='dummy prompt'\n",
        "guidance_scale=7.5\n",
        "eta=0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load images for inpainting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VAOMGL2NHaW0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/devdastl/EVA-8_Capstone_Assignment.git\n",
        "\n",
        "init_image = 'EVA-8_Capstone_Assignment/Part2-Inpainting_implementation/test_data/test_image.png'\n",
        "mask_image = 'EVA-8_Capstone_Assignment/Part2-Inpainting_implementation/test_data/mask_image.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Idn_6RoTHaW0"
      },
      "outputs": [],
      "source": [
        "# set timesteps\n",
        "accepts_offset = \"offset\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
        "extra_set_kwargs = {}\n",
        "offset = 0\n",
        "if accepts_offset:\n",
        "    offset = 1\n",
        "    extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwwVp1z2HaW0",
        "outputId": "b7914344-cbe1-4d14-f9c6-9c61fa1627f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512]) torch.Size([1, 3, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "batch = make_batch(init_image, mask_image, device)\n",
        "\n",
        "# preprocess image\n",
        "image = batch['image']\n",
        "mask = batch['mask']\n",
        "masked_image = batch['masked_image']\n",
        "print(image.shape, mask.shape, masked_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYRjyS1jHaW1",
        "outputId": "40fbbf96-f825-4878-f731-b25088a726f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 4, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    init_latent_dist = vae.encode(image).latent_dist\n",
        "    init_latents = init_latent_dist.sample(generator=generator)\n",
        "    init_latents = 0.18215 * init_latents\n",
        "    print(init_latents.shape)\n",
        "\n",
        "\n",
        "    latent_masked = vae.encode(masked_image).latent_dist.sample()\n",
        "    latent_masked = 0.18215 * latent_masked\n",
        "    print(latent_masked.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymijfRBiwn2U",
        "outputId": "a2807c20-b95a-4ab0-ffda-cb388fe30e57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 64, 64])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bchw = (1, 4, 64, 64)\n",
        "mask = torch.nn.functional.interpolate(mask, size=bchw[-2:])\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3QLu-Gsw0yS",
        "outputId": "0c45785e-5ff1-446b-ffce-77cb34b4b94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "init_latents_orig = init_latents\n",
        "maked_latents_unchange = torch.cat([latent_masked, mask], dim=1)\n",
        "#init_latents = torch.cat([init_latents, maked_latents_unchange], dim=1)\n",
        "print(init_latents.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aoQZ-GmqHaW1"
      },
      "outputs": [],
      "source": [
        "# get the original timestep using init_timestep\n",
        "init_timestep = int(num_inference_steps * strength) + offset\n",
        "init_timestep = min(init_timestep, num_inference_steps)\n",
        "timesteps = scheduler.timesteps[-init_timestep]\n",
        "timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "# add noise to latents using the timesteps\n",
        "noise = torch.randn(init_latents.shape, generator=generator, device=device)\n",
        "init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "# get prompt text embeddings\n",
        "text_input = tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "text_embeddings = text_encoder(text_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z418WJMvHaW1"
      },
      "outputs": [],
      "source": [
        "# here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "# of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "# corresponds to doing no classifier free guidance.\n",
        "do_classifier_free_guidance = guidance_scale > 1.0\n",
        "# get unconditional embeddings for classifier free guidance\n",
        "if do_classifier_free_guidance:\n",
        "    max_length = text_input.input_ids.shape[-1]\n",
        "    uncond_input = tokenizer(\n",
        "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
        "\n",
        "    # For classifier free guidance, we need to do two forward passes.\n",
        "    # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "    # to avoid doing two forward passes\n",
        "    #text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#unet = torch.nn.DataParallel(unet, device_ids=[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "fddd70c79f6049c68626ab9719b3d1dd",
            "5698268650d84102997522dbbee4bcef",
            "d841411507df468b9d532defaa02904e",
            "61501aee714540acbe9a4fb7dd841720",
            "831d2ca230f643f79d8778a73bbf2d5d",
            "0fc96af33ca34bc8a70aa7e4ac046e7f",
            "4fb094f3bd294af19123e39f6f3d2d74",
            "e22468e770a04f53bb5b71b4c5bb2801",
            "0f65b830719a42b5828b36ee89bbd63c",
            "00273c5dc5d443e5a5ebf32c390b1954",
            "14197ef66e79493fa33c40efc7e9e8ec"
          ]
        },
        "id": "dUew6BHKHaW2",
        "outputId": "7dba3844-5be6-4f9c-aa23-17b754834a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:01,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 9, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [00:01,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 9, 64, 64])\n",
            "torch.Size([1, 4, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4it [00:02,  2.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9, 64, 64])\n",
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 9, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6it [00:02,  4.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 9, 64, 64])\n",
            "torch.Size([1, 4, 64, 64])\n",
            "torch.Size([1, 9, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6it [00:02,  2.28it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 10.00 MiB (GPU 1; 15.90 GiB total capacity; 15.06 GiB already allocated; 5.75 MiB free; 15.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(latent_model_input\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 23\u001b[0m noise_pred \u001b[39m=\u001b[39m unet(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39;49mtext_embeddings)[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(noise_pred\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#if do_classifier_free_guidance:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m#    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m#    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py:773\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[1;32m    770\u001b[0m     upsample_size \u001b[39m=\u001b[39m down_block_res_samples[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(upsample_block, \u001b[39m\"\u001b[39m\u001b[39mhas_cross_attention\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m upsample_block\u001b[39m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 773\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    774\u001b[0m         hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    775\u001b[0m         temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m    776\u001b[0m         res_hidden_states_tuple\u001b[39m=\u001b[39;49mres_samples,\n\u001b[1;32m    777\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    778\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    779\u001b[0m         upsample_size\u001b[39m=\u001b[39;49mupsample_size,\n\u001b[1;32m    780\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    784\u001b[0m         hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb, res_hidden_states_tuple\u001b[39m=\u001b[39mres_samples, upsample_size\u001b[39m=\u001b[39mupsample_size\n\u001b[1;32m    785\u001b[0m     )\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py:1967\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   1961\u001b[0m                 create_custom_forward(attn, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1962\u001b[0m                 hidden_states,\n\u001b[1;32m   1963\u001b[0m                 encoder_hidden_states,\n\u001b[1;32m   1964\u001b[0m                 cross_attention_kwargs,\n\u001b[1;32m   1965\u001b[0m             )[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1966\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1967\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m   1968\u001b[0m         hidden_states \u001b[39m=\u001b[39m attn(\n\u001b[1;32m   1969\u001b[0m             hidden_states,\n\u001b[1;32m   1970\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1971\u001b[0m             cross_attention_kwargs\u001b[39m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m   1972\u001b[0m             return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1973\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/diffusers/models/resnet.py:595\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb)\u001b[0m\n\u001b[1;32m    593\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(hidden_states, temb)\n\u001b[1;32m    594\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(hidden_states)\n\u001b[1;32m    597\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinearity(hidden_states)\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[39m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/modules/normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgroup_norm(\n\u001b[1;32m    274\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_groups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[0;32m/workspace/capstone_assignment/Part2-inpainting/inpaint_env/lib/python3.8/site-packages/torch/nn/functional.py:2530\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2529\u001b[0m _verify_batch_size([\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups, num_groups] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:]))\n\u001b[0;32m-> 2530\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgroup_norm(\u001b[39minput\u001b[39;49m, num_groups, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.00 MiB (GPU 1; 15.90 GiB total capacity; 15.06 GiB already allocated; 5.75 MiB free; 15.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "# eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "# eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "# and should be between [0, 1]\n",
        "accepts_eta = \"eta\" in set(inspect.signature(scheduler.step).parameters.keys())\n",
        "extra_step_kwargs = {}\n",
        "if accepts_eta:\n",
        "    extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "init_latents = torch.cat([init_latents, maked_latents_unchange], dim=1).to('cuda:1')\n",
        "unet = unet.to('cuda:1')\n",
        "text_embeddings = text_embeddings.to('cuda:1')\n",
        "maked_latents_unchange = maked_latents_unchange.to('cuda:1')\n",
        "latents = init_latents \n",
        "\n",
        "t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "print(f't_start : {t_start}')\n",
        "for i, t in tqdm(enumerate(scheduler.timesteps[t_start:])):\n",
        "    # expand the latents if we are doing classifier free guidance\n",
        "    latent_model_input = torch.cat([latents] * 1) if do_classifier_free_guidance else latents\n",
        "\n",
        "    # predict the noise residual\n",
        "    #print(latent_model_input.shape)\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "    #print(noise_pred.shape)\n",
        "\n",
        "    # perform guidance\n",
        "    #if do_classifier_free_guidance:\n",
        "    #    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    #    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents[:,0:4:,:,:], **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "    # masking\n",
        "    #intermerdiate = scheduler.add_noise(maked_latents_unchange, noise, t)\n",
        "    latents = torch.cat([latents, maked_latents_unchange], dim=1)\n",
        "    del latent_model_input\n",
        "    del noise_pred\n",
        "    torch.cuda.empty_cache()\n",
        "    # latents = (init_latents_proper * mask) + (latents * (1 - mask))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00273c5dc5d443e5a5ebf32c390b1954": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f65b830719a42b5828b36ee89bbd63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fc96af33ca34bc8a70aa7e4ac046e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14197ef66e79493fa33c40efc7e9e8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fb094f3bd294af19123e39f6f3d2d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5698268650d84102997522dbbee4bcef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fc96af33ca34bc8a70aa7e4ac046e7f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fb094f3bd294af19123e39f6f3d2d74",
            "value": ""
          }
        },
        "61501aee714540acbe9a4fb7dd841720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00273c5dc5d443e5a5ebf32c390b1954",
            "placeholder": "​",
            "style": "IPY_MODEL_14197ef66e79493fa33c40efc7e9e8ec",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "831d2ca230f643f79d8778a73bbf2d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d841411507df468b9d532defaa02904e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e22468e770a04f53bb5b71b4c5bb2801",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f65b830719a42b5828b36ee89bbd63c",
            "value": 0
          }
        },
        "e22468e770a04f53bb5b71b4c5bb2801": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fddd70c79f6049c68626ab9719b3d1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5698268650d84102997522dbbee4bcef",
              "IPY_MODEL_d841411507df468b9d532defaa02904e",
              "IPY_MODEL_61501aee714540acbe9a4fb7dd841720"
            ],
            "layout": "IPY_MODEL_831d2ca230f643f79d8778a73bbf2d5d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
